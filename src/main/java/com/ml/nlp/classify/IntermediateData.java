package com.ml.nlp.classify;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.ObjectOutputStream;
import java.io.Serializable;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;

import com.ml.nlp.preprocessing.ChineseTokenizer;
import com.ml.nlp.preprocessing.TextTokenizer;


public class IntermediateData implements Serializable {
	/**
	 * auto generated by eclipse.
	 */
	private static final long serialVersionUID = -420389048890617397L;

	/** 类别名. */
    public String[] classifications;
    
    /** 单词X在类别C下出现的总数. */
	public HashMap[] filesOfXC;
	/** 给定分类下的文件数目. */
    public int[] filesOfC;
    /** 根目录下的文件总数. */
    public int files;
    
	/** 单词X在类别C下出现的总数 */
	public HashMap[] tokensOfXC;
    /** 类别C下所有单词的总数. */
    public int[] tokensOfC;
	
    /** 整个训练语料所出现的单词. */
    public Vocabulary vocabulary;
    /** 整个训练语料所出现的单词的IDF. */
    public Map<String, Double> termsIDF;
    
    
    /** 文本分类语料的根目录. */
    private transient String dir;
    /** 语料库中的文本文件的字符编码. */
    private transient String encoding;
    /** 中文分词. */
    private transient TextTokenizer textTokenizer;
    /** 限制语料库单词的数量. */
    private transient int featureSize = 50000;
	
    public IntermediateData() {
    	vocabulary = new Vocabulary();
    	textTokenizer = new ChineseTokenizer();
    }
    
    /**
     * 预计算，产生中间结果，存放到磁盘中.
     * 
     * @param trainTextDir
     *            已经分类的语料库，结构为 
     * trainnedTextDir 
     *            ├- 类别1\
     *                    ├- 文件1.txt 
     *                    ├- 文件2.txt ...
     *            ├- 类别2\
     *                    ├- 文件1.txt 
     *                    ├- 文件2.txt ...
     * @param txtEncoding
     *            语料库中的文本文件编码
     */
    public final void generate(final String trainTextDir,
            final String txtEncoding, final String modelFile) {
        // 一些初始化动作
        dir = trainTextDir;
        if (txtEncoding == null) {
            encoding = "GBK"; // 默认文本文件的编码为GBK;
        } else {
            encoding = txtEncoding;
        }

        // 枚举目录，获得各个类名
        File tmpDir = new File(dir);
        if (!tmpDir.isDirectory()) {
            throw new IllegalArgumentException("训练语料库搜索失败！ [" + dir
                    + "]");
        }
        classifications = tmpDir.list();
        
        filesOfC = new int[classifications.length];
        filesOfXC = new HashMap[classifications.length];
        tokensOfC = new int[classifications.length];
        tokensOfXC = new HashMap[classifications.length];
        for(int i = 0; i < classifications.length; i++) {
        	tokensOfXC[i] = new HashMap<String, Integer>();
        	filesOfXC[i] = new HashMap<String, Integer>();
        }
        
        // 计算各个类别的文件总数，保存
        for (int i = 0; i < classifications.length; i++) {
            int n = calcFileCountOfClassification(i);
            filesOfC[i] = n;
            files += n; // 计算文件总数，保存
        }
        
		// 获得单词表。计算各类别下单词总数，单词总数。 计算各个单词的IDF
        try {
			calculate();
		} catch (IOException e1) {
			e1.printStackTrace();
		}
		
		// 将预处理后的数据写入到磁盘
		try {
            ObjectOutputStream out = new ObjectOutputStream(
                    new FileOutputStream(modelFile));
            out.writeObject(this);
            out.close();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    
    /**
     * 返回训练文本集中在给定分类下的训练文本数目.
     * 
     * @param c
     *            给定的分类
     * @return 训练文本集中在给定分类下的训练文本数目
     */
    private int calcFileCountOfClassification(
            final int c) {
        File classDir = new File(dir + File.separator
                + classifications[c]);
        return classDir.list().length;
    }
    
    /**
     * 计算 fileCountOfXC, fileCountOfC, fileCount, tokensOfXC, tokensOfC, tokens.
     * 
     * @throws IOException
     */
    private void calculate() throws IOException {
    	List<List<String>> documents = new ArrayList<List<String>>();
        for (int i = 0;i < classifications.length; i++) {
        	HashMap<String, Integer> tmpT = (HashMap<String, Integer>)tokensOfXC[i];
        	HashMap<String, Integer> tmpF = (HashMap<String, Integer>)filesOfXC[i];
        	
            String[] filesPath = getFilesPath(dir, classifications[i]);
            
            filesOfC[i] = filesPath.length;
            files += filesOfC[i];

            HashSet<String> words = new HashSet<String>();
            for (String file : filesPath) {
            	words.clear();
                String text = getText(file, encoding);
                
                List<String> terms = null;
                // 中文分词处理(分词后结果可能还包含有停用词）
                terms = textTokenizer.tokenize(text, " ");
                terms = textTokenizer.dropStopWords(terms); // 去掉停用词，以免影响分类
                
                // 获得单词表
                vocabulary.addAll(terms);
                
                // 添加文档，以后面计算IDF
                documents.add(terms);
        		
                for (String term : terms) { // 计算本类别下每个单词的出现次数
                	Integer value = tmpT.get(term);
                    if(value == null) {
                    	tmpT.put(term, new Integer(1));
                    } else {
                    	tmpT.put(term, value + 1);
                    }
                    
                    //去除重复单词
                    words.add(term);
                }
                
                // 开始计算 filesOfXC[i]
				for (Iterator<String> iter = words.iterator(); iter.hasNext();) {
					String key = iter.next();
					Integer value = tmpF.get(key);
					if (value == null) {
						tmpF.put(key, new Integer(1));
					} else {
						value++;
						tmpF.put(key, value);
					}
				}
            }
            
            // 该类别下所有单词的出现总数 nC
            for (Iterator<Entry<String, Integer>> iter = tmpT.entrySet()
                    .iterator(); iter.hasNext();) {
                Entry<String, Integer> entry = iter.next();

                tokensOfC[i] += entry.getValue().intValue();
            }
            
            
        }
        
        vocabulary.limitWords(featureSize);
        System.out.println("calculate...");
        long start = System.currentTimeMillis();
        //calculateIDF(documents);
        long end = System.currentTimeMillis();
        System.out.println("idf time:" + (end -start));

    }
    
	private void calculateIDF(List<List<String>> documents) {
		// IDF＝log( |D| / (|Dt| + 1))，其中|D|表示文档总数，|Dt|表示包含关键词t的文档数量
		// 分母之所以要加1，是为了避免分母为0
		this.termsIDF = new HashMap<String, Double>(vocabulary.wordCount());
		int i = 0;
		for (String term : vocabulary) {
			double d = 0.0;
			for (List<String> document : documents) {
				if (document.contains(term)) {
					d++;
				}
			}
			double idf = Math.log((double) files / (double) (1 + d));
			this.termsIDF.put(term, idf);
			if(i%10000 == 0) {
				System.out.println(i);
			}
			
			i++;
		}

	}
    
    /**
     * 根据训练文本类别返回这个类别下的所有训练文本路径(full path).
     * 
     * @param dirStr 已分类的文本根目录，末尾不带斜杠
     * @param classification
     *            给定的分类
     * @return 给定分类下所有文件的路径（full path）
     */
    public static String[] getFilesPath(final String dirStr, final String classification) {
        File classDir = new File(dirStr + File.separator
                + classification);
        String[] ret = classDir.list();
        for (int i = 0; i < ret.length; i++) {
            ret[i] = dirStr + File.separator
                    + classification + File.separator + ret[i];
        }
        return ret;
    }


    /**
     * 返回给定路径的文本文件内容.
     * 
     * @param filePath
     *            给定的文本文件路径
     * @param encoding 文本文件的编码
     * @return 文本内容
     * @throws java.io.IOException
     *             文件找不到或IO出错
     */
    public static String getText(final String filePath, final String encoding)
            throws IOException {

        InputStreamReader isReader = new InputStreamReader(new FileInputStream(
                filePath), encoding);
        BufferedReader reader = new BufferedReader(isReader);
        String aLine;
        StringBuilder sb = new StringBuilder();

        while ((aLine = reader.readLine()) != null) {
            sb.append(aLine + " ");
        }
        isReader.close();
        reader.close();
        return sb.toString();
    }
    
    /** 打印命令行参数的解释信息. */
    private static void usage() {
        System.err.println("usage:\t  <语料库目录> <语料库文本编码> <中间文件>");
    }
    
    /**
     * 使用方法：IntermediateData d:\SogouC.mini\Sample\ gbk d:\mini.db
     * @param args
     */
    public static void main(String args[]) {
    	if(args.length < 3) {
    		usage();
    		//return;
    	}
    	IntermediateData tdm = new IntermediateData();
    	//tdm.generate(args[0], args[1], args[2]);
		String fileDir = "C:\\soft\\dm\\text_classification";

    	tdm.generate("C:\\soft\\TrainingSet2", "gbk", fileDir + "\\intermediate2.db");
    	System.out.println("中间数据生成！");
    }
}